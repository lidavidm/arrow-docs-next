.. Licensed to the Apache Software Foundation (ASF) under one
.. or more contributor license agreements.  See the NOTICE file
.. distributed with this work for additional information
.. regarding copyright ownership.  The ASF licenses this file
.. to you under the Apache License, Version 2.0 (the
.. "License"); you may not use this file except in compliance
.. with the License.  You may obtain a copy of the License at

..   http://www.apache.org/licenses/LICENSE-2.0

.. Unless required by applicable law or agreed to in writing,
.. software distributed under the License is distributed on an
.. "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
.. KIND, either express or implied.  See the License for the
.. specific language governing permissions and limitations
.. under the License.

.. default-domain:: cpp
.. highlight:: cpp

================
Tabular Datasets
================

.. seealso::
   :doc:`Dataset API reference <api/dataset>`

.. warning::

    The ``arrow::dataset`` namespace is experimental, and a stable API
    is not yet guaranteed.

The Arrow Datasets library provides functionality to efficiently work with
tabular, potentially larger than memory and multi-file datasets:

* A unified interface for different sources: supporting different sources and
  file formats (Parquet, Feather files) and different file systems (local,
  cloud).
* Discovery of sources (crawling directories, handle directory-based partitioned
  datasets, basic schema normalization, ..)
* Optimized reading with predicate pushdown (filtering rows), projection
  (selecting columns), parallel reading, or fine-grained managing of tasks.

Currently, only Parquet, Feather / Arrow IPC, and CSV files are supported. The
goal is to expand this in the future to other file formats and data sources
(e.g.  database connections).

Reading Datasets
----------------

For the examples below, let's create a small dataset consisting
of a directory with two parquet files:

.. literalinclude:: ../../../cpp/examples/arrow/dataset-documentation-example.cc
   :language: cpp
   :lines: 50-85
   :linenos:
   :lineno-match:

(See the full example at bottom: :ref:`cpp-dataset-full-example`.)

Dataset discovery
~~~~~~~~~~~~~~~~~

A :class:`arrow::dataset::Dataset` object can be created using the various
:class:`arrow::dataset::DatasetFactory` objects. Here, we'll use the
:class:`arrow::dataset::FileSystemDatasetFactory`, which can create a dataset
given the base directory path:

.. literalinclude:: ../../../cpp/examples/arrow/dataset-documentation-example.cc
   :language: cpp
   :lines: 151-170
   :emphasize-lines: 6-11
   :linenos:
   :lineno-match:

Here, we're also passing the filesystem to use and the file format to
read. This lets us choose between things like reading local files or files in
Amazon S3, or between Parquet and CSV.

In addition to a base directory path, we can list file paths manually.

Creating a :class:`arrow::dataset::Dataset` in this way loads nothing into
memory; it only crawls the directory to find all the files
(:func:`arrow::dataset::FileSystemDataset::files`):

.. code-block:: cpp

   for (const auto& filename : dataset->files()) {
     std::cout << filename << std::endl;
   }

â€¦and infers the dataset's schema (by default from the first file):

.. code-block:: cpp

   std::cout << dataset->schema()->ToString() << std::endl;

Using the :func:`arrow::dataset::Dataset::NewScan` method, we can build a
:class:`arrow::dataset::Scanner` and read the dataset (or a portion of it) into
a :class:`arrow::Table` with the :func:`arrow::dataset::Scanner::ToTable`
method:

.. literalinclude:: ../../../cpp/examples/arrow/dataset-documentation-example.cc
   :language: cpp
   :lines: 151-170
   :emphasize-lines: 16-19
   :linenos:
   :lineno-match:

.. TODO: iterative loading not documented pending API changes
.. note:: Depending on the size of your dataset, this can require a lot of
          memory; see :ref:`cpp-dataset-filtering-data` below on
          filtering/projecting.

Reading different file formats
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

The above examples use Parquet files on local disk, but the Dataset API
provides a consistent interface across multiple file formats and sources.
Currently, Parquet, Feather / Arrow IPC, and CSV file formats are supported;
more formats are planned in the future.

If we save the table as a Feather file instead of Parquet files:

.. literalinclude:: ../../../cpp/examples/arrow/dataset-documentation-example.cc
   :language: cpp
   :lines: 87-104
   :linenos:
   :lineno-match:

then we can read the Feather file using the same functions, but passing a
:class:`arrow::dataset::IpcFileFormat`:

.. literalinclude:: ../../../cpp/examples/arrow/dataset-documentation-example.cc
   :language: cpp
   :lines: 310,323
   :linenos:

Customizing file formats
~~~~~~~~~~~~~~~~~~~~~~~~

:class:`arrow::dataset::FileFormat` objects have properties that control how
they are read. For example::

  auto format = std::make_shared<ds::ParquetFileFormat>();
  format->reader_options.dict_columns.insert("a");

Will configure column ``"a"`` to be dictionary-encoded when read.

.. _cpp-dataset-filtering-data:

Filtering data
--------------

So far, we've read the entire dataset, but this can be inefficient or use too
much memory. The :class:`arrow::dataset::Scanner` offers control over what data
to read.

:func:`arrow::dataset::ScannerBuilder::Project` lets us read only specified
columns:

.. literalinclude:: ../../../cpp/examples/arrow/dataset-documentation-example.cc
   :language: cpp
   :lines: 172-191
   :emphasize-lines: 16
   :linenos:
   :lineno-match:

For formats which support pushdown (such as Parquet), this means that only the
specified columns will be read from the filesystem, saving I/O costs.

A filter can be provided with :func:`arrow::dataset::ScannerBuilder::Filter`,
so that rows which do not match the filter predicate will not be included in
the returned table.

.. literalinclude:: ../../../cpp/examples/arrow/dataset-documentation-example.cc
   :language: cpp
   :lines: 172-191
   :emphasize-lines: 17
   :linenos:
   :lineno-match:

.. TODO Expressions not documented pending renamespacing

Projecting columns
------------------

In addition to selecting columns,
:func:`arrow::dataset::ScannerBuilder::Project` can also be used for more
complex projections in combination with expressions.

In this case, we pass a vector of expressions used to construct column values,
and a vector of names for the columns:

.. literalinclude:: ../../../cpp/examples/arrow/dataset-documentation-example.cc
   :language: cpp
   :lines: 193-223
   :emphasize-lines: 18-28
   :linenos:
   :lineno-match:

This also determines the column selection; only the given columns will be
present in the resulting table. If you want to include a derived column in
*addition* to the existing columns, you can build up the expressions from the
dataset schema:

.. literalinclude:: ../../../cpp/examples/arrow/dataset-documentation-example.cc
   :language: cpp
   :lines: 225-254
   :emphasize-lines: 17-27
   :linenos:
   :lineno-match:

Reading and writing partitioned data
------------------------------------

So far, we've been working with datasets consisting of flat directories with
files. However, a dataset can exploit nested directory structures defining a
partitioned dataset, where sub-directory names hold information about which
subset of the data is stored in that directory.

For example, a dataset partitioned by year and month may look like on disk:

.. code-block:: text

   dataset_name/
     year=2007/
       month=01/
          data0.parquet
          data1.parquet
          ...
       month=02/
          data0.parquet
          data1.parquet
          ...
       month=03/
       ...
     year=2008/
       month=01/
       ...
     ...

The above partitioning scheme is using "/key=value/" directory names, as found
in Apache Hive.

Let's create a small partitioned dataset. For this, we'll use Dataset's writing
functionality.

.. literalinclude:: ../../../cpp/examples/arrow/dataset-documentation-example.cc
   :language: cpp
   :lines: 106-149
   :emphasize-lines: 25-42
   :linenos:
   :lineno-match:

The above created a directory with two subdirectories ("part=a" and "part=b"),
and the Parquet files written in those directories no longer include the "part"
column.

Reading this dataset, we now specify that the dataset uses a Hive-like
partitioning scheme:

.. literalinclude:: ../../../cpp/examples/arrow/dataset-documentation-example.cc
   :language: cpp
   :lines: 256-279
   :emphasize-lines: 7,9-11
   :linenos:
   :lineno-match:

Although the partition fields are not included in the actual Parquet files,
they will be added back to the resulting table when scanning this dataset:

.. code-block:: text

   $ ./debug/dataset-documentation-example file:///tmp parquet_hive partitioned
   Found fragment: /tmp/parquet_dataset/part=a/part0.parquet
   Partition expression: (part == "a")
   Found fragment: /tmp/parquet_dataset/part=b/part1.parquet
   Partition expression: (part == "b")
   Read 20 rows
   a: int64
     -- field metadata --
     PARQUET:field_id: '1'
   b: double
     -- field metadata --
     PARQUET:field_id: '2'
   c: int64
     -- field metadata --
     PARQUET:field_id: '3'
   part: string
   ----
   # snip...

We can now filter on the partition keys, which avoids loading files
altogether if they do not match the predicate:

.. literalinclude:: ../../../cpp/examples/arrow/dataset-documentation-example.cc
   :language: cpp
   :lines: 281-301
   :emphasize-lines: 15-18
   :linenos:
   :lineno-match:

Different partitioning schemes
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

The above example uses a hive-like directory scheme, such as "/year=2009/month=11/day=15".
We specified this by passing the partition *factory*. In this case, the types of
the partition keys are inferred from the file paths.

It is also possible directly construct the partitioning and explicitly define
the schema of the partition keys. For example:

.. code-block:: cpp

    auto part = std::make_shared<ds::HivePartitioning>(arrow::schema({
        arrow::field("year", arrow::int16()),
        arrow::field("month", arrow::int8()),
        arrow::field("day", arrow::int32())
    }));

"Directory partitioning" is also supported, where the segments in the file path
represent the values of the partition keys without including the name (the
field name are implicit in the segment's index). For example, given field names
"year", "month", and "day", one path might be "/2019/11/15".

Since the names are not included in the file paths, these must be specified
when constructing a directory partitioning:

.. code-block:: cpp

    auto part = ds::DirectoryPartitioning::MakeFactory({"year", "month", "day"});

Directory partitioning also supports providing a full schema rather than inferring
types from file paths.

Reading from other data sources
-------------------------------

Reading in-memory data
~~~~~~~~~~~~~~~~~~~~~~

If you already have data in memory that you'd like to use with the Datasets
interface (e.g. to filter/project data, or to write it out to a filesystem),
you can wrap it in a :class:`arrow::dataset::InMemoryDataset`:

.. code-block:: cpp

   auto table = arrow::Table::FromRecordBatches(...);
   auto dataset = std::make_shared<arrow::dataset::InMemoryDataset>(std::move(table));
   // Scan the dataset, filter, it, etc.
   auto scanner_builder = dataset->NewScan();

In the example, we used the InMemoryDataset to write our example data to local
disk to be used in the rest of the example:

.. literalinclude:: ../../../cpp/examples/arrow/dataset-documentation-example.cc
   :language: cpp
   :lines: 106-149
   :emphasize-lines: 24-28
   :linenos:
   :lineno-match:

Reading from cloud storage
~~~~~~~~~~~~~~~~~~~~~~~~~~

In addition to local files, Arrow Datasets also supports reading from cloud
storage by passing a different filesystem.

See the :ref:`filesystem <cpp-filesystems>` docs for more details on the available
filesystems.

.. _cpp-dataset-full-example:

Full Example
------------

.. literalinclude:: ../../../cpp/examples/arrow/dataset-documentation-example.cc
   :language: cpp
   :linenos:
